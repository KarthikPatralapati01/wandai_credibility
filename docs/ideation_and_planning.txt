
IDEATION + PLANNING
=====================

ğŸ¯ Problem Overview
---------------------
Large Language Models (LLMs) synthesize information from diverse sources. However, credibility varies depending on the source (e.g., peer-reviewed study vs. corporate PR), content (e.g., superlatives vs. data-backed facts), and context (e.g., self-promotion vs. independent verification). This system evaluates and continuously updates the credibility of extracted claims, especially as new sources (like Pitchbook reports) are added.

ğŸ“¦ Solution Architecture
--------------------------
- [Source Ingestion]
    â†’ Read all `.txt` files from `data/` (can be gov docs, news articles, PRs, etc.)
    â†’ Each source gets metadata: type, publisher, path, date

- [Claim Extraction Agent]
    â†’ Applies heuristic rules to extract factual/promotional/numeric claims
    â†’ Annotates tone and claim type

- [Feature Engineering + Scoring Engine]
    â†’ Computes features per claim: source_prior, linguistic, contextuality, recency, conflict-of-interest
    â†’ Assigns credibility score [0,1] + band (HIGH, MEDIUM, LOW)

- [Corroboration Engine]
    â†’ Embeds claims using SentenceTransformers (or token cosine fallback)
    â†’ Finds best corroborating evidence from *different publishers*
    â†’ Boosts credibility if independently corroborated

- [Incremental Update Engine]
    â†’ When a new file is added:
        â–ª Extract new claims
        â–ª Embed them
        â–ª Match against previous claims (similarity > threshold)
        â–ª Re-score only affected claims
        â–ª Generate new scores, before â†’ after deltas, and reasons using GPT
        â–ª Summarize updated credibility research

- [Reporting]
    â†’ Full research report (`report_initial.txt`, `report_incremental.txt`)
    â†’ Human-readable summary of only affected claims (`incremental_summary.txt`)

ğŸ¤– Agent Design
--------------------------
- Extraction Agent: Parses and annotates each source's claims
- Scoring Agent: Computes weighted credibility features and assigns band
- Corroboration Agent: Embeds and compares semantic similarity
- Update Agent: Selectively updates only impacted claims after new evidence
- LLM Explainer: Generates 1-sentence human-readable reasons per change
- Summary Agent: Writes natural language summary of overall impact

ğŸ“Š Goals / KPIs
--------------------------
- Coverage: % of sources from which valid claims are extracted
- Precision: % of high-scoring claims aligned with independent sources
- Change Detection: % of claims updated after adding new sources
- Runtime: Execution time for incremental vs full recomputation
- LLM Cost: Number of GPT calls per update

ğŸ— Infrastructure Sketch
--------------------------
- File-based ingestion (MVP) using plain `.txt`
- Embedding with SentenceTransformers (MiniLM); fallback to lexical cosine
- All state stored as JSON and .txt reports
- Designed for command-line execution, easily upgradable to web API

ğŸ§ª Edge Cases Handled
--------------------------
- Conflicting claims from new sources â†’ score reduced (with contradiction-based logic)
- Redundant claims â†’ not re-counted for corroboration
- Promotional superlatives from ads/PR â†’ penalized in score
- No claims found â†’ graceful fallback
- No matching prior claims â†’ new claims added, none updated
- LLM not available â†’ disables incremental mode (Option 2)

